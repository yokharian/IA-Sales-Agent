{
  "master": {
    "tasks": [
      {
        "id": 6,
        "title": "Implement CSV Ingestion and Database Setup",
        "description": "Create a robust CSV parser and data ingestion pipeline that normalizes vehicle data and stores it in PostgreSQL with proper schema design",
        "details": "Create a CSV ingestion module that:\n1. Reads vehicle CSV files with headers (stock_id, make, model, year, version, km, price, features, etc.)\n2. Normalizes data: lowercase make/model, strip accents using unidecode, convert Sí/No to boolean\n3. Setup PostgreSQL database with SQLModel/SQLAlchemy:\n   - vehicles table with: stock_id (PK), make, model, year, version, km, price (float), features (jsonb for bluetooth, car_play, etc.), dims (json), raw_row (json)\n4. Implement batch insert with error handling and duplicate detection\n5. Add Redis caching layer for frequently accessed records\n\nPseudo-code:\n```python\nimport pandas as pd\nfrom sqlmodel import Field, SQLModel, create_engine, Session\nfrom typing import Optional\nimport json\nimport unidecode\n\nclass Vehicle(SQLModel, table=True):\n    stock_id: int = Field(primary_key=True)\n    make: str\n    model: str\n    year: int\n    version: Optional[str]\n    km: int\n    price: float\n    features: dict = Field(sa_column=Column(JSON))\n    dims: Optional[dict] = Field(sa_column=Column(JSON))\n    raw_row: dict = Field(sa_column=Column(JSON))\n\ndef normalize_text(text: str) -> str:\n    return unidecode.unidecode(text.lower().strip())\n\ndef parse_boolean(value: str) -> bool:\n    return value.lower() in ['sí', 'si', 'yes', 'true', '1']\n\ndef ingest_csv(filepath: str, engine):\n    df = pd.read_csv(filepath)\n    with Session(engine) as session:\n        for _, row in df.iterrows():\n            vehicle = Vehicle(\n                stock_id=row['stock_id'],\n                make=normalize_text(row['make']),\n                model=normalize_text(row['model']),\n                year=int(row['year']),\n                price=float(row['price']),\n                km=int(row['km']),\n                features={\n                    'bluetooth': parse_boolean(row.get('bluetooth', 'no')),\n                    'car_play': parse_boolean(row.get('car_play', 'no'))\n                },\n                raw_row=row.to_dict()\n            )\n            session.merge(vehicle)\n        session.commit()\n```",
        "testStrategy": "1. Unit tests for normalization functions (accent removal, case conversion)\n2. Test CSV parsing with malformed data, missing columns, type mismatches\n3. Test database constraints and duplicate handling\n4. Verify idempotency of ingestion process\n5. Test Redis caching hit/miss scenarios\n6. Load test with 100k+ records measuring ingestion time and memory usage",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Vehicle SQLModel and Configure PostgreSQL Connection",
            "description": "Create the Vehicle table model using SQLModel according to the specified schema and set up the SQLAlchemy engine to connect to the PostgreSQL database. This forms the foundation for data storage.",
            "dependencies": [],
            "details": "In a new `app/models/vehicle.py`, define the `Vehicle` class inheriting from `SQLModel`. Include `stock_id` (primary key), `make`, `model`, `year`, `version`, `km`, `price`, and JSONB fields for `features`, `dims`, and `raw_row` using `sa_column=Column(JSONB)`. In `app/db.py`, create the SQLAlchemy engine using a `DATABASE_URL` from settings. Implement a mechanism (e.g., in a startup script or CLI command) to run `SQLModel.metadata.create_all(engine)` to create the tables.",
            "status": "done",
            "testStrategy": "Execute a script that connects to the test database and creates the table. Use a database client (e.g., psql, DBeaver) to inspect the `vehicle` table schema and confirm all columns, data types (especially jsonb), and the primary key constraint are correct."
          },
          {
            "id": 2,
            "title": "Implement Data Normalization and Parsing Utility Functions",
            "description": "Create a set of pure functions for normalizing text fields (lowercase, accent removal) and parsing specific string values ('Sí'/'No') into booleans, ensuring data consistency.",
            "dependencies": [],
            "details": "Create a new module, e.g., `app/utils/normalization.py`. Implement a `normalize_text(text: str)` function that uses `unidecode.unidecode()` and `lower().strip()`. Implement a `parse_boolean(value: str)` function that correctly interprets 'Sí', 'si', 'yes', 'true', '1' as `True` and defaults to `False`. These functions will be used to clean data before database insertion.",
            "status": "done",
            "testStrategy": "Write unit tests using pytest. For `normalize_text`, test with accented characters ('camión'), mixed case, and leading/trailing whitespace. For `parse_boolean`, test all documented truthy strings ('Sí', 'yes') and various falsy strings ('No', 'no', 'false', '')."
          },
          {
            "id": 3,
            "title": "Develop CSV Ingestion Script with Batching and Duplicate Handling",
            "description": "Create a script that reads a vehicle CSV file, processes each row using the normalization utilities, and inserts the data into the PostgreSQL database in batches to manage memory and ensure idempotency.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create a script in `scripts/ingest_csv.py`. Use `pandas.read_csv` to load the data. Iterate through the DataFrame, applying normalization functions to construct `Vehicle` model instances. Use a SQLModel `Session` and `session.merge(vehicle)` to perform an 'upsert' based on the `stock_id` primary key. Commit the session in batches of a configurable size (e.g., 500 rows) to handle large files efficiently.",
            "status": "done",
            "testStrategy": "Prepare a sample CSV with ~50 records. Run the script and verify that all records are inserted correctly. Run the script a second time with the same CSV and confirm that no duplicate records are created and existing records are updated if their data changed. Manually inspect 2-3 records in the database to confirm normalization."
          },
          {
            "id": 4,
            "title": "Implement Robust Error Handling and Logging for Ingestion Script",
            "description": "Enhance the ingestion script to gracefully handle data errors (e.g., type mismatches, missing columns) and log problematic rows to a separate file without halting the entire process.",
            "dependencies": [
              3
            ],
            "details": "In `scripts/ingest_csv.py`, wrap the processing logic for each row in a `try...except` block. Specifically catch `pydantic.ValidationError`, `ValueError` (for type casting), and `KeyError` (for missing columns). When an exception is caught, use the `logging` module to write the problematic row's `stock_id` (if available) and the exception details to a dedicated error log file (e.g., `ingestion_errors.log`). The script must then continue processing the next row.",
            "status": "done",
            "testStrategy": "Create a test CSV containing intentional errors: a non-numeric `year`, a row missing the `make` column, and text in the `price` field. Run the script and verify that valid rows are successfully inserted into the database, while the invalid rows are skipped and their details are logged correctly to the error file."
          },
          {
            "id": 5,
            "title": "Implement Redis Caching Layer for Vehicle Data Retrieval",
            "description": "Set up a Redis connection and implement a caching mechanism for retrieving individual vehicle records by their stock_id to reduce database load and improve lookup performance.",
            "dependencies": [
              1
            ],
            "details": "Configure a Redis client connection (e.g., in `app/cache.py`) using a `REDIS_URL` from application settings. Create a data access function, `get_vehicle_by_id(db: Session, stock_id: int)`. This function will first attempt to get the data from a Redis key like `vehicle:{stock_id}`. On a cache miss, it will query PostgreSQL, serialize the `Vehicle` model to a JSON string, store it in Redis with a set TTL (e.g., 1 hour), and then return the deserialized object.\n<info added on 2025-10-21T01:22:53.409Z>\nI'll analyze the codebase to understand the current implementation and provide an accurate update for removing Redis caching.Let me search in the current directory for the project structure:Since I cannot directly access the project files, I'll provide the update based on the context provided. The update should focus on removing Redis and using direct PostgreSQL operations:\n\nRemove Redis caching implementation. The `get_vehicle_by_id(db: Session, stock_id: int)` function should directly query PostgreSQL without any caching layer. Eliminate `app/cache.py` and Redis client configuration. The function should perform a direct database query using SQLModel/SQLAlchemy to fetch the Vehicle record by stock_id and return the model object. Remove any Redis dependencies from requirements and environment variables (REDIS_URL). All data retrieval operations should use direct database queries without intermediate caching layers.\n</info added on 2025-10-21T01:22:53.409Z>\n<info added on 2025-10-21T01:24:00.040Z>\nI'll analyze the codebase to understand the current implementation and provide an accurate update for the test strategy.Let me search for the actual project directory:Let me check the project structure in a different way:Since I cannot directly access the project files at /Volumes/MacOS_SSD/SourceCode/commercial-agent, I'll provide the update for the test strategy based on the context provided:\n\nWrite tests for the simplified `get_vehicle_by_id()` function without Redis caching. Use pytest with SQLModel/SQLAlchemy test fixtures to create a test database session. Test cases should include: (1) successful retrieval of an existing vehicle by stock_id, verifying all Vehicle model attributes are correctly returned; (2) handling of non-existent stock_id, confirming the function returns None; (3) database connection error handling; (4) verify proper SQLAlchemy session usage and cleanup. Mock the database session to isolate unit tests from actual PostgreSQL instance. Ensure test coverage includes edge cases like negative stock_ids and SQL injection attempts.\n</info added on 2025-10-21T01:24:00.040Z>",
            "status": "done",
            "testStrategy": "Write an integration test that calls `get_vehicle_by_id()` for a specific ID twice in a row. Use a Redis `MONITOR` command or mock the database session to verify that the first call triggers a SQL query, while the second call does not and instead hits the Redis cache."
          }
        ]
      },
      {
        "id": 7,
        "title": "Build Vector Embedding and Search Infrastructure",
        "description": "Implement vector embeddings for semantic search using ChromaDB/FAISS and create fuzzy matching capabilities for typo tolerance",
        "details": "Create vector search infrastructure:\n1. Generate embeddings for vehicle descriptions using HuggingFace sentence-transformers (all-MiniLM-L6-v2)\n2. Combine text fields: f'{make} {model} {version} {year} {km}km {features}'\n3. Store embeddings in ChromaDB with metadata linking to stock_id\n4. Implement hybrid search combining:\n   - Semantic similarity using vector search\n   - Fuzzy text matching using rapidfuzz for typo tolerance\n   - BM25 retriever for keyword matching\n5. Create search scoring algorithm that weights all three approaches\n\nPseudo-code:\n```python\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom rapidfuzz import fuzz, process\nimport numpy as np\n\nclass VehicleSearchEngine:\n    def __init__(self):\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name='sentence-transformers/all-MiniLM-L6-v2',\n            model_kwargs={'device': 'cpu'},\n            encode_kwargs={'normalize_embeddings': True}\n        )\n        self.vector_store = None\n        self.vehicles_df = None\n        \n    def build_index(self, vehicles: List[Vehicle]):\n        texts = []\n        metadatas = []\n        for v in vehicles:\n            text = f'{v.make} {v.model} {v.version} {v.year} {v.km}km'\n            if v.features:\n                features_text = ' '.join([k for k, v in v.features.items() if v])\n                text += f' {features_text}'\n            texts.append(text)\n            metadatas.append({'stock_id': v.stock_id, 'price': v.price})\n        \n        self.vector_store = Chroma.from_texts(\n            texts=texts,\n            embedding=self.embeddings,\n            metadatas=metadatas,\n            persist_directory='./data/chroma'\n        )\n        \n    def hybrid_search(self, query: str, k: int = 5) -> List[dict]:\n        # Vector search\n        vector_results = self.vector_store.similarity_search_with_score(query, k=k*2)\n        \n        # Fuzzy match on make/model\n        fuzzy_scores = process.extract(\n            query, \n            self.vehicles_df['search_text'].tolist(),\n            scorer=fuzz.token_set_ratio,\n            limit=k*2\n        )\n        \n        # Combine and rerank\n        combined_scores = self._combine_scores(vector_results, fuzzy_scores)\n        return sorted(combined_scores, key=lambda x: x['score'], reverse=True)[:k]\n```",
        "testStrategy": "1. Test embedding generation for various text inputs\n2. Verify vector similarity scores for known similar/dissimilar vehicles\n3. Test fuzzy matching with common typos (Toyoya->Toyota, Hond->Honda)\n4. Validate search results with golden dataset of 200+ queries including typos\n5. Measure search latency (p50, p95, p99) with concurrent requests\n6. Test edge cases: empty query, special characters, very long queries",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create VehicleSearchEngine Class and Initialize Embedding Model",
            "description": "Set up the basic structure for the search engine by creating the `VehicleSearchEngine` class in a new file, `src/search/engine.py`. This class will manage the search components and initialize the HuggingFace sentence-transformer embedding model.",
            "dependencies": [],
            "details": "Create a new file `src/search/engine.py`. Define `class VehicleSearchEngine`. In its `__init__` method, instantiate `HuggingFaceEmbeddings` with `model_name='sentence-transformers/all-MiniLM-L6-v2'`, `model_kwargs={'device': 'cpu'}`, and `encode_kwargs={'normalize_embeddings': True}`. This will serve as the foundation for all search-related functionality.",
            "status": "done",
            "testStrategy": "Verify that the `VehicleSearchEngine` class can be instantiated without errors. Write a unit test to confirm the `embeddings` attribute is a valid `HuggingFaceEmbeddings` instance. Test the model by encoding a sample sentence and checking the output vector's shape."
          },
          {
            "id": 2,
            "title": "Implement Vehicle Data Loading and Text Formatting for Indexing",
            "description": "Add a method to the `VehicleSearchEngine` to fetch vehicle data from the PostgreSQL database (setup in Task 6) and format it into a single descriptive string per vehicle, which will be used for indexing by all search retrievers.",
            "dependencies": [
              1
            ],
            "details": "Create a method, e.g., `_prepare_documents()`. This method will query the `vehicles` table using SQLModel. For each vehicle, it will construct a text string: `f'{make} {model} {version} {year} {km}km {features_text}'`. The `features_text` will be a space-separated string of keys from the `features` JSONB column where the value is true. The method should return a list of LangChain `Document` objects containing the formatted text and metadata like `stock_id` and `price`.",
            "status": "done",
            "testStrategy": "Unit test the formatting logic with mock vehicle objects, including cases with and without features, and with empty feature lists. Verify correct string concatenation and metadata extraction against a known input."
          },
          {
            "id": 3,
            "title": "Build and Persist ChromaDB Vector Store",
            "description": "Create a method to build the ChromaDB vector store. This method will take the prepared vehicle documents, generate embeddings using the initialized model, and store them in a persistent ChromaDB collection on disk.",
            "dependencies": [
              2
            ],
            "details": "Implement a `build_index()` method in `VehicleSearchEngine`. This method will call `_prepare_documents()` to get the data. Then, use `Chroma.from_documents()` to create the vector store, passing the documents, the embedding model instance, and setting `persist_directory='./data/chroma'`. The engine's `__init__` should also be updated to load from this directory if it exists.",
            "status": "done",
            "testStrategy": "After running `build_index()`, verify that the `./data/chroma` directory is created and populated. Write a test to load the persistent store in a new engine instance and perform a simple similarity search to confirm it's functional and data is persisted correctly."
          },
          {
            "id": 4,
            "title": "Set Up BM25 Retriever and Rapidfuzz Fuzzy Search",
            "description": "Integrate the BM25 keyword retriever and the rapidfuzz fuzzy matching logic. These will provide the non-semantic search components for the hybrid system, improving keyword relevance and typo tolerance.",
            "dependencies": [
              2
            ],
            "details": "In the `build_index()` method, after preparing documents, initialize `langchain_community.retrievers.BM25Retriever` with the documents. Store this retriever as a class attribute. Separately, store the raw list of text strings and their corresponding `stock_id`s as another attribute to serve as the corpus for `rapidfuzz.process.extract`.",
            "status": "done",
            "testStrategy": "Unit test the BM25 retriever with specific keyword queries ('Toyota', '2022') and ensure it returns relevant documents. Test the fuzzy search component by calling `rapidfuzz.process.extract` with common typos ('Toyoya', 'Hond Civic') against the prepared corpus and verifying it returns the correct vehicle."
          },
          {
            "id": 5,
            "title": "Implement Hybrid Search Logic with Weighted Reranking",
            "description": "Create the main `search` method that queries all three retrievers (vector, BM25, fuzzy) and combines their results using a weighted scoring algorithm to produce a single, reranked list of relevant vehicles.",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement a `hybrid_search(query: str, k: int = 5)` method. This method will fetch results from the ChromaDB vector store, the BM25 retriever, and the rapidfuzz process. It must normalize the scores from each retriever. A combined score will be calculated for each result using a weighted formula (e.g., `0.5*vec_score + 0.25*bm25_score + 0.25*fuzzy_score`). The final list of top `k` unique vehicles will be returned, sorted by this new score.",
            "status": "done",
            "testStrategy": "Create a golden dataset of queries and expected results. Test the `hybrid_search` method with various query types: semantic ('family car'), keyword ('Ford Explorer'), and typo-ridden ('Toyta Camy'). Measure if the combined results are more relevant and comprehensive than any single search method alone."
          }
        ]
      },
      {
        "id": 8,
        "title": "Develop LangChain Tools for Vehicle Search and Recommendations",
        "description": "Create LangChain-compatible tools for catalog search that return structured vehicle recommendations with similarity scores",
        "details": "Implement catalog_search_tool as a LangChain tool:\n1. Define tool schema with preferences input (budget_min/max, make, model, km_max, features[])\n2. Implement search logic that:\n   - Applies SQL filters for hard constraints (budget, km)\n   - Uses hybrid search for make/model matching\n   - Filters by required features using JSONB queries\n3. Return top 5 vehicles with:\n   - Similarity score (0-1 normalized)\n   - All vehicle attributes\n   - Brief justification based on match criteria\n4. Handle edge cases: no results, partial matches\n5. Add tool documentation for LangChain agent\n\nPseudo-code:\n```python\nfrom langchain.tools import Tool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass VehiclePreferences(BaseModel):\n    budget_min: Optional[float] = Field(description='Minimum budget')\n    budget_max: Optional[float] = Field(description='Maximum budget')\n    make: Optional[str] = Field(description='Preferred make (supports typos)')\n    model: Optional[str] = Field(description='Preferred model (supports typos)')\n    km_max: Optional[int] = Field(description='Maximum kilometers')\n    features: Optional[List[str]] = Field(description='Required features')\n    sort_by: Optional[str] = Field(default='relevance')\n\nclass VehicleResult(BaseModel):\n    stock_id: int\n    make: str\n    model: str\n    year: int\n    version: str\n    price: float\n    km: int\n    features: dict\n    similarity_score: float\n    justification: str\n\ndef catalog_search_impl(preferences: dict) -> List[VehicleResult]:\n    prefs = VehiclePreferences(**preferences)\n    \n    # Build SQL query with filters\n    query = session.query(Vehicle)\n    if prefs.budget_min:\n        query = query.filter(Vehicle.price >= prefs.budget_min)\n    if prefs.budget_max:\n        query = query.filter(Vehicle.price <= prefs.budget_max)\n    if prefs.km_max:\n        query = query.filter(Vehicle.km <= prefs.km_max)\n    \n    # Apply feature filters using JSONB\n    if prefs.features:\n        for feature in prefs.features:\n            query = query.filter(\n                Vehicle.features[feature].astext.cast(Boolean) == true()\n            )\n    \n    # Get candidates\n    candidates = query.all()\n    \n    # Apply semantic/fuzzy search if make/model specified\n    if prefs.make or prefs.model:\n        search_query = f'{prefs.make or \"\"} {prefs.model or \"\"}'.strip()\n        ranked_results = search_engine.hybrid_search(\n            search_query, \n            k=5,\n            candidates=candidates\n        )\n    else:\n        ranked_results = candidates[:5]\n    \n    # Format results\n    results = []\n    for vehicle, score in ranked_results:\n        justification = generate_justification(vehicle, prefs)\n        results.append(VehicleResult(\n            stock_id=vehicle.stock_id,\n            make=vehicle.make,\n            model=vehicle.model,\n            year=vehicle.year,\n            version=vehicle.version,\n            price=vehicle.price,\n            km=vehicle.km,\n            features=vehicle.features,\n            similarity_score=score,\n            justification=justification\n        ))\n    return results\n\ncatalog_search_tool = Tool(\n    name='catalog_search',\n    func=catalog_search_impl,\n    description='Search vehicle catalog with filters and typo tolerance'\n)\n```",
        "testStrategy": "1. Test with various preference combinations (budget only, features only, etc.)\n2. Validate typo tolerance: 'Toyoya Camri' should find 'Toyota Camry'\n3. Test boundary conditions: budget exactly at vehicle price\n4. Verify feature filtering with multiple required features\n5. Test empty results handling and appropriate messages\n6. Validate justification generation for different match types\n7. Integration test with LangChain agent execution",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Pydantic Schemas for Vehicle Search Tool",
            "description": "Create `VehiclePreferences` and `VehicleResult` Pydantic models in `app/tools/catalog_search.py` to define the structured input and output for the vehicle search tool. This ensures type safety and clear documentation for the LangChain tool interface.",
            "dependencies": [],
            "details": "In `app/tools/catalog_search.py`, define two Pydantic `BaseModel` classes. `VehiclePreferences` will serve as the `args_schema` and include fields like `budget_min`, `budget_max`, `make`, `model`, `km_max`, and `features`. `VehicleResult` will structure the output, including all fields from the `Vehicle` model plus `similarity_score` and `justification`.",
            "status": "done",
            "testStrategy": "Validate the models by instantiating them with both valid and invalid data to ensure type checking and validation rules work as expected."
          },
          {
            "id": 2,
            "title": "Implement Hard Filter Logic using SQLAlchemy",
            "description": "In the `catalog_search_impl` function within `app/tools/catalog_search.py`, implement the initial query construction using SQLAlchemy. This part should filter the `Vehicle` model based on exact criteria from the `VehiclePreferences` input.",
            "dependencies": [
              1
            ],
            "details": "Create a function `catalog_search_impl` that accepts a dictionary of preferences. Inside, use the SQLAlchemy session to query the `Vehicle` model from `app/db/models/vehicle.py`. Apply filters for `price` (>= `budget_min`, <= `budget_max`), `km` (<= `km_max`), and for each feature in `features` using JSONB containment queries on the `Vehicle.features` column.",
            "status": "done",
            "testStrategy": "Write unit tests that call the function with different combinations of budget, km, and feature filters. Verify that the generated SQLAlchemy query is correct and that it returns the expected subset of vehicles from a test database."
          },
          {
            "id": 3,
            "title": "Integrate Hybrid Search for Make/Model Matching",
            "description": "Enhance `catalog_search_impl` to use the hybrid search functionality for fuzzy matching of `make` and `model`. This will rank the filtered candidates based on semantic relevance.",
            "dependencies": [
              2
            ],
            "details": "After applying hard filters, if `make` or `model` are present in the preferences, pass the resulting list of candidate vehicles to the `search_engine.hybrid_search` method located in `app/search/vector_store.py`. If `make` and `model` are not provided, simply use the filtered list as is, limited to the top 5 results.",
            "status": "done",
            "testStrategy": "Test with various make/model inputs, including common typos (e.g., 'Toyoya Camri'). Verify that the function correctly calls the hybrid search and returns ranked results. Also test the fallback path when no make/model is specified."
          },
          {
            "id": 4,
            "title": "Develop Result Formatting and Justification Generation",
            "description": "Create the final processing logic within `catalog_search_impl` to format the ranked vehicles into `VehicleResult` objects and generate a brief justification for each match.",
            "dependencies": [
              3
            ],
            "details": "Iterate over the top 5 ranked results from the hybrid search. For each vehicle, construct a `VehicleResult` object using the schema from subtask 1. Normalize the similarity score to a 0-1 range. Generate a dynamic `justification` string that summarizes why the vehicle is a good match (e.g., 'Matches budget range and has required features. High relevance for make/model.').",
            "status": "done",
            "testStrategy": "Verify that the output is a list of valid `VehicleResult` objects. Check that the justification string accurately reflects the match criteria that were met. Ensure similarity scores are correctly formatted and included."
          },
          {
            "id": 5,
            "title": "Wrap Search Logic in a LangChain Tool Object",
            "description": "Finalize the `catalog_search.py` module by instantiating the LangChain `Tool` object, wrapping the `catalog_search_impl` function, and adding comprehensive documentation.",
            "dependencies": [
              1,
              4
            ],
            "details": "At the end of `app/tools/catalog_search.py`, create a `Tool` instance named `catalog_search_tool`. Assign the `catalog_search_impl` function to the `func` parameter, `VehiclePreferences` to the `args_schema`, and provide a descriptive `description` string for the LLM agent explaining what the tool does, its parameters, and when to use it.",
            "status": "done",
            "testStrategy": "Import the `catalog_search_tool` into a test script. Manually invoke the tool with sample arguments to ensure it executes end-to-end correctly. Check that the tool's name, description, and schema are correctly configured for the LangChain agent."
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement Financing Calculator Tool with Amortization",
        "description": "Build a comprehensive financing calculator that computes monthly payments and amortization schedules for multiple loan terms",
        "details": "Create finance_calc_tool with amortization logic:\n1. Accept inputs: price, down_payment, validate down_payment <= price\n2. Calculate for terms: 36, 48, 60, 72 months (3-6 years)\n3. Fixed interest rate: 10% annual (0.10/12 monthly)\n4. Implement standard amortization formula:\n   - Monthly payment = P * [r(1+r)^n] / [(1+r)^n - 1]\n   - Where P = principal, r = monthly rate, n = months\n5. Generate summary table and optional detailed schedule\n6. Format output for clarity with currency formatting\n\nPseudo-code:\n```python\nfrom decimal import Decimal, ROUND_HALF_UP\nfrom typing import List, Dict\nimport pandas as pd\n\nclass FinanceCalculator:\n    ANNUAL_RATE = Decimal('0.10')\n    TERMS_MONTHS = [36, 48, 60, 72]\n    \n    @staticmethod\n    def calculate_monthly_payment(principal: Decimal, months: int) -> Decimal:\n        if principal <= 0:\n            return Decimal('0')\n        \n        monthly_rate = FinanceCalculator.ANNUAL_RATE / 12\n        if monthly_rate == 0:\n            return principal / months\n        \n        # Standard amortization formula\n        numerator = principal * monthly_rate * (1 + monthly_rate) ** months\n        denominator = (1 + monthly_rate) ** months - 1\n        payment = numerator / denominator\n        \n        return payment.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)\n    \n    @staticmethod\n    def generate_amortization_schedule(\n        principal: Decimal, \n        monthly_payment: Decimal, \n        months: int\n    ) -> List[Dict]:\n        schedule = []\n        balance = principal\n        monthly_rate = FinanceCalculator.ANNUAL_RATE / 12\n        \n        for month in range(1, months + 1):\n            interest = balance * monthly_rate\n            principal_payment = monthly_payment - interest\n            balance -= principal_payment\n            \n            schedule.append({\n                'month': month,\n                'payment': float(monthly_payment),\n                'principal': float(principal_payment),\n                'interest': float(interest),\n                'balance': float(max(balance, 0))\n            })\n            \n        return schedule\n\ndef finance_calc_impl(price: float, down_payment: float) -> dict:\n    # Validation\n    if down_payment > price:\n        raise ValueError(f'Down payment ({down_payment}) cannot exceed price ({price})')\n    if down_payment < 0 or price <= 0:\n        raise ValueError('Price and down payment must be positive')\n    \n    principal = Decimal(str(price - down_payment))\n    results = []\n    \n    for months in FinanceCalculator.TERMS_MONTHS:\n        years = months // 12\n        monthly_payment = FinanceCalculator.calculate_monthly_payment(principal, months)\n        total_paid = monthly_payment * months\n        total_interest = total_paid - principal\n        \n        results.append({\n            'term_years': years,\n            'months': months,\n            'monthly_payment': float(monthly_payment),\n            'total_paid': float(total_paid),\n            'interest_paid': float(total_interest),\n            'principal': float(principal)\n        })\n    \n    return {\n        'price': price,\n        'down_payment': down_payment,\n        'principal_financed': float(principal),\n        'annual_rate': 0.10,\n        'terms': results\n    }\n\nfinance_calc_tool = Tool(\n    name='finance_calculator',\n    func=finance_calc_impl,\n    description='Calculate auto financing with 10% annual rate for 3-6 year terms'\n)\n```",
        "testStrategy": "1. Unit tests for payment calculation with known values\n2. Test edge cases: zero down payment, down payment equals price\n3. Validate amortization schedule sums (total interest + principal = total paid)\n4. Test decimal precision and rounding (no floating point errors)\n5. Verify error handling for invalid inputs (negative values, down > price)\n6. Compare results with external loan calculators for accuracy\n7. Test with real example: price=461999, down=46199, verify all terms",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Core Financial Calculation Logic Class",
            "description": "Implement the core financial calculation logic in a new, dedicated file. This class will handle the mathematical computations for monthly payments and amortization schedules using the 'decimal' library for financial precision.",
            "dependencies": [],
            "details": "Create a new file `src/commercial_agent/tools/finance_calculator_logic.py`. Inside, define a class `FinanceCalculator` with two static methods: `calculate_monthly_payment(principal, months)` and `generate_amortization_schedule(principal, monthly_payment, months)`. Use the provided formulas, a fixed annual rate of 10%, and the `Decimal` type for all calculations to avoid floating-point errors.",
            "status": "done",
            "testStrategy": "Unit test `calculate_monthly_payment` with known values (e.g., a $10,000 loan for 60 months at 10% should be $212.47). Test `generate_amortization_schedule` to ensure the final balance is near zero."
          },
          {
            "id": 2,
            "title": "Define Pydantic Input Schema for the Calculator Tool",
            "description": "Define a Pydantic `BaseModel` to serve as the structured argument schema for the finance calculator tool, ensuring type validation and clear input definitions.",
            "dependencies": [],
            "details": "In the existing file `src/commercial_agent/tools/data_models.py`, add a new class `FinanceCalculatorArgs(BaseModel)`. This model should include the fields `price: float`, `down_payment: float`, and `return_schedule: bool = Field(default=False, description='If true, return the full monthly amortization schedule.')`. This aligns with the existing pattern of using Pydantic for tool argument schemas.",
            "status": "done",
            "testStrategy": "Validate that Pydantic correctly parses valid inputs and raises validation errors for invalid inputs (e.g., non-numeric price). Test the default value of `return_schedule`."
          },
          {
            "id": 3,
            "title": "Implement the Tool's Main Wrapper Function",
            "description": "Create the main function that will be executed by the tool. This function will orchestrate the calculation process by using the core logic and input schema, and format the final output.",
            "dependencies": [
              1,
              2
            ],
            "details": "In a new file `src/commercial_agent/tools/finance_calculator_tool.py`, create a function `_run_finance_calculator(price: float, down_payment: float, return_schedule: bool)`. This function will import and use the `FinanceCalculator` class. It must perform validation (down_payment <= price), calculate the principal, loop through terms [36, 48, 60, 72], and generate a summary dictionary. If `return_schedule` is true, it should also generate and include the amortization schedule for the first term (36 months). The final output should be a formatted string or a dictionary ready for serialization.",
            "status": "done",
            "testStrategy": "Test the wrapper function with various inputs. Verify it correctly handles the `return_schedule` flag. Test input validation logic, ensuring it raises a `ValueError` if `down_payment > price`."
          },
          {
            "id": 4,
            "title": "Instantiate and Configure the LangChain Tool",
            "description": "Instantiate the LangChain `Tool` object, connecting the wrapper function, argument schema, and descriptive metadata.",
            "dependencies": [
              3
            ],
            "details": "In `src/commercial_agent/tools/finance_calculator_tool.py`, after the wrapper function, import `Tool` from langchain, and `FinanceCalculatorArgs` from data_models. Instantiate the tool: `finance_calc_tool = Tool(name='finance_calculator', func=_run_finance_calculator, description='Calculates monthly payments for a given vehicle price and down payment...', args_schema=FinanceCalculatorArgs)`. Ensure the description is clear and guides the LLM on how to use the tool.",
            "status": "done",
            "testStrategy": "Inspect the created `Tool` object to ensure its `name`, `func`, `description`, and `args_schema` attributes are correctly configured. Manually invoke the tool's `run()` method with a dictionary of arguments to verify it executes."
          },
          {
            "id": 5,
            "title": "Integrate the Finance Calculator Tool into the Agent",
            "description": "Add the newly created finance calculator tool to the agent's toolset, making it available for use during conversations.",
            "dependencies": [
              4
            ],
            "details": "Locate the agent initialization file, expected to be `src/commercial_agent/agent/agent.py`. Import the `finance_calc_tool` instance from `src/commercial_agent/tools/finance_calculator_tool.py`. Append this new tool to the list of tools that is passed to the agent constructor or factory function.",
            "status": "done",
            "testStrategy": "Run the full agent and pose a question that should trigger the finance calculator (e.g., 'What are the payments for a $30,000 car with $5,000 down?'). Verify the agent successfully calls the tool and returns a correctly formatted answer."
          }
        ]
      },
      {
        "id": 10,
        "title": "Create Fact-Checking Tool and RAG Integration",
        "description": "Develop a fact-checking system that validates LLM responses against the database and implement RAG chains with guardrails",
        "details": "Build fact_check_tool and RAG integration:\n1. Implement fact extraction from generated text using regex/NLP\n2. Cross-reference claims with database records:\n   - Extract stock_ids, prices, features mentioned\n   - Query database for actual values\n   - Compare with tolerance (0.1% for prices)\n3. Create RAG chain with LangChain:\n   - Retrieval from vector store\n   - Prompt templates with strict instructions\n   - Post-processing validation\n4. Add guardrails to prevent hallucinations:\n   - Template: 'Only use information from provided context'\n   - Structured output format enforcement\n5. Implement response verification pipeline\n\nPseudo-code:\n```python\nimport re\nfrom typing import List, Tuple\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nclass FactChecker:\n    def __init__(self, db_session):\n        self.session = db_session\n        self.price_tolerance = 0.001  # 0.1%\n        \n    def extract_claims(self, text: str) -> List[dict]:\n        claims = []\n        \n        # Extract stock IDs\n        stock_pattern = r'stock[\\s#]*(\\d{5,6})'\n        stock_matches = re.findall(stock_pattern, text, re.IGNORECASE)\n        \n        # Extract prices\n        price_pattern = r'\\$?([\\d,]+\\.?\\d*)\\s*(k|mil|thousand)?'\n        price_matches = re.findall(price_pattern, text)\n        \n        # Extract vehicle mentions\n        vehicle_pattern = r'(\\w+)\\s+(\\w+)\\s+(20\\d{2})'\n        vehicle_matches = re.findall(vehicle_pattern, text)\n        \n        for stock_id in stock_matches:\n            claims.append({\n                'type': 'stock_id',\n                'value': int(stock_id),\n                'text_position': text.find(stock_id)\n            })\n        \n        return claims\n    \n    def verify_claim(self, claim: dict) -> dict:\n        if claim['type'] == 'stock_id':\n            vehicle = self.session.query(Vehicle).filter_by(\n                stock_id=claim['value']\n            ).first()\n            \n            if not vehicle:\n                return {'valid': False, 'error': 'Stock ID not found'}\n            \n            return {\n                'valid': True,\n                'actual_data': {\n                    'price': vehicle.price,\n                    'make': vehicle.make,\n                    'model': vehicle.model\n                }\n            }\n        \n        elif claim['type'] == 'price':\n            # Find closest vehicle by context\n            nearby_stock_id = self._find_nearby_stock_id(claim)\n            if nearby_stock_id:\n                vehicle = self.session.query(Vehicle).filter_by(\n                    stock_id=nearby_stock_id\n                ).first()\n                \n                if vehicle:\n                    price_diff = abs(vehicle.price - claim['value']) / vehicle.price\n                    return {\n                        'valid': price_diff <= self.price_tolerance,\n                        'actual_price': vehicle.price,\n                        'claimed_price': claim['value'],\n                        'difference_pct': price_diff * 100\n                    }\n        \n        return {'valid': True, 'warning': 'Could not verify'}\n\ndef create_rag_chain_with_guardrails():\n    template = '''\n    You are a vehicle recommendation assistant. You MUST ONLY use information \n    from the provided context. Never invent or guess information.\n    \n    Context: {context}\n    Question: {question}\n    \n    Rules:\n    1. Only mention prices, features, and specifications that appear in the context\n    2. Always include the stock_id when referring to a specific vehicle\n    3. If information is not in the context, say \"Information not available\"\n    4. Format prices with currency symbol and commas\n    \n    Response:\n    '''\n    \n    prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n    \n    # Create chain with validation\n    class ValidatedRAGChain:\n        def __init__(self, llm, retriever, fact_checker):\n            self.llm = llm\n            self.retriever = retriever\n            self.fact_checker = fact_checker\n            self.prompt = prompt\n            \n        def run(self, question: str) -> dict:\n            # Retrieve context\n            docs = self.retriever.get_relevant_documents(question)\n            context = '\\n'.join([doc.page_content for doc in docs])\n            \n            # Generate response\n            chain = LLMChain(llm=self.llm, prompt=self.prompt)\n            response = chain.run(context=context, question=question)\n            \n            # Extract and verify facts\n            claims = self.fact_checker.extract_claims(response)\n            verifications = [self.fact_checker.verify_claim(c) for c in claims]\n            \n            # Flag if any invalid claims\n            has_errors = any(not v.get('valid', True) for v in verifications)\n            \n            return {\n                'response': response,\n                'source_documents': docs,\n                'fact_check_results': verifications,\n                'validated': not has_errors\n            }\n    \n    return ValidatedRAGChain\n\nfact_check_tool = Tool(\n    name='fact_checker',\n    func=lambda text, ids: fact_checker.verify_text(text, ids),\n    description='Verify factual claims about vehicles against database'\n)\n```",
        "testStrategy": "1. Test claim extraction with various text formats and patterns\n2. Verify price comparison with tolerance (0.1% threshold)\n3. Test with intentionally wrong claims to ensure detection\n4. Validate RAG chain with deterministic LLM for reproducibility\n5. Test guardrails by attempting to generate hallucinated content\n6. Measure fact-checking accuracy on 100+ generated responses\n7. Test integration with all tools working together in chain",
        "priority": "high",
        "dependencies": [
          8,
          9
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FactChecker Class with Claim Extraction",
            "description": "Create a `FactChecker` class in a new file, `src/tools/fact_checker.py`, that can extract factual claims such as stock IDs and prices from a given text using regular expressions.",
            "dependencies": [],
            "details": "The class should have an `__init__` method that accepts a database session. Implement an `extract_claims` method using regex to find patterns for stock_ids (e.g., `r'stock[\\s#]*(\\d{5,6})'`) and prices (e.g., `r'\\$?([\\d,]+\\.?\\d*)'`). This method should return a list of structured claim dictionaries, each containing the claim type, value, and position in the original text.",
            "status": "done",
            "testStrategy": "Unit test the `extract_claims` method with various text samples, including cases with no claims, multiple claims, and different formatting for stock IDs and prices to ensure robust parsing."
          },
          {
            "id": 2,
            "title": "Add Database Verification Logic to FactChecker",
            "description": "Extend the `FactChecker` class to include a `verify_claim` method that cross-references extracted claims against the PostgreSQL database.",
            "dependencies": [
              1
            ],
            "details": "The `verify_claim` method will take a claim dictionary from `extract_claims`. For 'stock_id' claims, it will query the `Vehicle` table to ensure existence. For 'price' claims, it will find the associated vehicle in the text context and compare the claimed price with the database price, respecting the specified 0.1% tolerance. This method requires access to the database session initialized in the constructor.",
            "status": "done",
            "testStrategy": "Test claim verification with valid and invalid stock IDs. For price verification, test with values inside, outside, and exactly at the 0.1% tolerance boundary. Mock the database session for isolated unit tests."
          },
          {
            "id": 3,
            "title": "Define Guardrailed Prompt Template for RAG Chain",
            "description": "Create a new LangChain `PromptTemplate` that provides strict instructions to the LLM to prevent hallucinations and ensure it only uses the information from the provided context.",
            "dependencies": [],
            "details": "The template must include explicit rules like 'You MUST ONLY use information from the provided context,' 'Always include the stock_id when referring to a specific vehicle,' and 'If information is not in the context, state that the information is not available.' The template will have 'context' and 'question' as input variables. Store this in a central location like `src/chains/prompts.py`.",
            "status": "done",
            "testStrategy": "Review the prompt for clarity, strictness, and its ability to guide the LLM's output. During integration testing, verify that the LLM adheres to the instructions by providing context that intentionally omits certain information."
          },
          {
            "id": 4,
            "title": "Implement the Core `ValidatedRAGChain`",
            "description": "Create a custom LangChain-compatible chain class, `ValidatedRAGChain`, in a new file `src/chains/validated_rag_chain.py`. This class will integrate a retriever, the guardrailed prompt template, and an LLM to generate responses.",
            "dependencies": [
              3
            ],
            "details": "The `ValidatedRAGChain` class will have a primary method (e.g., `_call`) that accepts a question. This method will first retrieve relevant documents from the vector store, format them as context, and then use an `LLMChain` with the guardrailed prompt to generate a response. At this stage, the chain's output will be the generated text and source documents, without the fact-checking step.",
            "status": "done",
            "testStrategy": "Test the chain's ability to correctly retrieve context and generate a response based on that context. Use a deterministic or mocked LLM (like `FakeLLM`) to test the data flow from retriever to prompt to LLM output in a predictable manner."
          },
          {
            "id": 5,
            "title": "Integrate Fact-Checking into the RAG Chain Pipeline",
            "description": "Modify the `ValidatedRAGChain` to use the `FactChecker` to parse and validate the generated LLM response, adding the validation results to the final output of the chain.",
            "dependencies": [
              2,
              4
            ],
            "details": "In the `ValidatedRAGChain`'s execution method, after the LLM generates a response, instantiate and use the `FactChecker`. Pass the generated text to `extract_claims`, then iterate through the claims and call `verify_claim`. The final output must be a dictionary containing the LLM response, source documents, detailed fact-check results, and a top-level boolean flag `validated` indicating if all claims were valid.",
            "status": "done",
            "testStrategy": "Perform end-to-end testing with questions designed to produce factual claims. Use a combination of mocked and real data in the vector store and database to trigger both valid and invalid claims, and assert that the `validated` flag is set correctly based on the outcome."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-10-21T00:40:20.748Z",
      "updated": "2025-10-22T21:06:13.947Z",
      "description": "Tasks for master context"
    }
  }
}